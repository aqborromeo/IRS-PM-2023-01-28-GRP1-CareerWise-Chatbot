{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b940994",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: net::ERR_ADDRESS_UNREACHABLE\n  (Session info: chrome=111.0.5563.110)\nStacktrace:\n#0 0x55dd948c1243 <unknown>\n#1 0x55dd946857a6 <unknown>\n#2 0x55dd9467d728 <unknown>\n#3 0x55dd94670772 <unknown>\n#4 0x55dd94671e82 <unknown>\n#5 0x55dd94670b3a <unknown>\n#6 0x55dd9466fbf8 <unknown>\n#7 0x55dd9466fa30 <unknown>\n#8 0x55dd9466e508 <unknown>\n#9 0x55dd9466eb3d <unknown>\n#10 0x55dd94687596 <unknown>\n#11 0x55dd946fbfc5 <unknown>\n#12 0x55dd946e3082 <unknown>\n#13 0x55dd946fb932 <unknown>\n#14 0x55dd946e2e53 <unknown>\n#15 0x55dd946b59ea <unknown>\n#16 0x55dd946b6b2e <unknown>\n#17 0x55dd94915d5e <unknown>\n#18 0x55dd94919a80 <unknown>\n#19 0x55dd948fb8b0 <unknown>\n#20 0x55dd9491ab63 <unknown>\n#21 0x55dd948ecf75 <unknown>\n#22 0x55dd9493d998 <unknown>\n#23 0x55dd9493db27 <unknown>\n#24 0x55dd94958c23 <unknown>\n#25 0x7f0cd8894b43 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.mycareersfuture.sg/search?sortBy=new_posting_date&page=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(page)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#search=research%20scientist&sortBy=new_posting_date&page=2'\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m sleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     18\u001b[0m html \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:449\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: net::ERR_ADDRESS_UNREACHABLE\n  (Session info: chrome=111.0.5563.110)\nStacktrace:\n#0 0x55dd948c1243 <unknown>\n#1 0x55dd946857a6 <unknown>\n#2 0x55dd9467d728 <unknown>\n#3 0x55dd94670772 <unknown>\n#4 0x55dd94671e82 <unknown>\n#5 0x55dd94670b3a <unknown>\n#6 0x55dd9466fbf8 <unknown>\n#7 0x55dd9466fa30 <unknown>\n#8 0x55dd9466e508 <unknown>\n#9 0x55dd9466eb3d <unknown>\n#10 0x55dd94687596 <unknown>\n#11 0x55dd946fbfc5 <unknown>\n#12 0x55dd946e3082 <unknown>\n#13 0x55dd946fb932 <unknown>\n#14 0x55dd946e2e53 <unknown>\n#15 0x55dd946b59ea <unknown>\n#16 0x55dd946b6b2e <unknown>\n#17 0x55dd94915d5e <unknown>\n#18 0x55dd94919a80 <unknown>\n#19 0x55dd948fb8b0 <unknown>\n#20 0x55dd9491ab63 <unknown>\n#21 0x55dd948ecf75 <unknown>\n#22 0x55dd9493d998 <unknown>\n#23 0x55dd9493db27 <unknown>\n#24 0x55dd94958c23 <unknown>\n#25 0x7f0cd8894b43 <unknown>\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import random\n",
    "soup_list = []\n",
    "page = 0\n",
    "while True:\n",
    "    \n",
    "    service_obj = Service(\"./drivers/chromedriver\")\n",
    "    driver = webdriver.Chrome(service=service_obj)\n",
    "    url = 'https://www.mycareersfuture.sg/search?sortBy=new_posting_date&page='+str(page)\n",
    "\n",
    "    #search=research%20scientist&sortBy=new_posting_date&page=2'\n",
    "\n",
    "    driver.get(url)\n",
    "    sleep(2)\n",
    "    html = driver.page_source\n",
    "    soup_list.append(BeautifulSoup(html, 'lxml'))\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    driver.close()\n",
    "    page+=1\n",
    "    if (page == 1000):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee7c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889\n"
     ]
    }
   ],
   "source": [
    "# Initialized all variables with empty lists\n",
    "job_title = []\n",
    "company_name = []\n",
    "job_location = []\n",
    "job_level = []\n",
    "salary = []\n",
    "salary_min = []\n",
    "salary_max = []\n",
    "salary_type = []\n",
    "days_posted = []\n",
    "job_desc = []\n",
    "job_req = []\n",
    "job_type = []\n",
    "job_cat = []\n",
    "job_url = []\n",
    "job_count = 0\n",
    "jip_count = 0\n",
    "is_responsive = []\n",
    "is_contract = []\n",
    "is_intern = []\n",
    "is_temp = []\n",
    "is_perm = []\n",
    "link_list = []    # initialise list to hold the links of job postings\n",
    "job_class_list = []\n",
    "timer = random.randint(2, 5)\n",
    "\n",
    "minreq = 15\n",
    "print(len(soup_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612a8a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Get the links of job postings on the results page for the job listings\n",
    "#class_list = soup.find_all('a', {'class': 'bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3'})\n",
    "# initialise dataframe to hold the url list \n",
    "data_df = pd.DataFrame()\n",
    "for soup in soup_list:\n",
    "    \n",
    "    list_of_jobs = soup.findAll(\"p\", {'data-testid':'company-hire-info'})\n",
    "    for x in range(len(list_of_jobs)):\n",
    "        company_name.append(list_of_jobs[x].text)\n",
    "\n",
    "    # Job Title\n",
    "    job_title_var = soup.findAll(\"span\", {'data-cy':'job-card__job-title'})\n",
    "    for x in range(len(job_title_var)):\n",
    "        job_title.append(job_title_var[x].text)\n",
    "\n",
    "    # Job Links\n",
    "    job_link_var = soup.findAll('a', {'data-testid':'job-card-link'})     \n",
    "    for x in range(len(job_link_var)):\n",
    "        job_class_list.append(job_link_var[x][\"href\"])\n",
    "        job_url.append(\"https://www.mycareersfuture.sg\"+job_link_var[x][\"href\"])   \n",
    "\n",
    "\n",
    "    #print('==== List of job postings url ====')\n",
    "    #print('='*35)        \n",
    "    #for i in range(len(list_of_jobs)):\n",
    "    #    print(\"Company: \" + company_name[i])\n",
    "    #    print(\"Job Title: \" + job_title[i])\n",
    "    #    print(\"Job URL: \" + job_url[i])\n",
    "        #print(\"job class: \" + job_class_list[i][\"href\"])\n",
    "i=0\n",
    "for job in list_of_jobs:    \n",
    "    job_info = [{\n",
    "        'company' : job.get_text(),\n",
    "        'job_title' : job_title[i],\n",
    "        'url' : job_url[i]\n",
    "    }]\n",
    "    i+=1\n",
    "\n",
    "    # append the job posting to dataframe\n",
    "    temp_df = pd.DataFrame(job_info)\n",
    "    data_df = pd.concat([data_df, temp_df], axis=0)\n",
    "data_df.to_csv('job_info_list.csv', encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0582e021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17780\n"
     ]
    }
   ],
   "source": [
    "print(len(job_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3193ca75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6244/2418609643.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver_job = webdriver.Chrome(executable_path=\"./drivers/chromedriver\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i=0\n",
    "# iterate through the link_list to load each job posting\n",
    "for url in job_url:\n",
    "#    if (i>10):\n",
    "#        break\n",
    "            \n",
    "    driver_job = webdriver.Chrome(executable_path=\"./drivers/chromedriver\")\n",
    "    driver_job.get(url)\n",
    "    sleep(1)\n",
    "    posting = driver_job.page_source\n",
    "    \n",
    "    job_soup = BeautifulSoup(posting, 'lxml')\n",
    "    #print(job_soup.prettify())\n",
    "    \n",
    "    # get company address \n",
    "    job_location_var = job_soup.find('p', {'id':'address'})\n",
    "    if (job_location_var):\n",
    "        job_location.append(job_location_var.get_text())\n",
    "    else:\n",
    "        job_location.append(\"Unknown\") \n",
    "\n",
    "    # Job type \n",
    "    job_level_var = job_soup.find(\"p\", {'id':'employment_type'})\n",
    "    if (job_level_var):\n",
    "        job_type.append(job_level_var.get_text())\n",
    "    else:\n",
    "        job_type.append(\"Unknown\")\n",
    "\n",
    "    # Job Seniority (if any?)\n",
    "    job_level_var = job_soup.find(\"p\", {'id':'seniority'})\n",
    "    if (job_level_var):\n",
    "        job_level.append(job_level_var.get_text())\n",
    "    else:\n",
    "        job_level.append(\"Unknown\")\n",
    "\n",
    "    # Job Category\n",
    "    job_level_var = job_soup.find(\"p\", {'id':'job-categories'})\n",
    "    if (job_level_var):\n",
    "        job_cat.append(job_level_var.get_text())\n",
    "    else:\n",
    "        job_cat.append(\"Unknown\")\n",
    "\n",
    "    # find salary \n",
    "    salary_range = job_soup.find_all('div', {'class':'lh-solid'})\n",
    "    for element in salary_range:\n",
    "        salary.append(element.get_text())\n",
    "    if (salary):\n",
    "        salary_list = salary[i].split(\"to\")\n",
    "        if (len(salary_list) == 1):\n",
    "            salary_min.append(salary_list[0])\n",
    "            salary_max.append(salary_list[0])\n",
    "        elif (len(salary_list) == 2):\n",
    "            salary_min.append(salary_list[0])\n",
    "            salary_max.append(salary_list[1])\n",
    "        \n",
    "    # find salary type\n",
    "    job_info_list = job_soup.find('span', {'data-testid' : 'salary-type'})\n",
    "    if (job_info_list):\n",
    "        salary_type.append(job_info_list.get_text())\n",
    "    else:\n",
    "        salary_type.append(\"Unknown\")\n",
    "\n",
    "    # getting the job description\n",
    "    job_desc_section = job_soup.find('div', {'id':'description-content'})\n",
    "    if (job_desc_section):\n",
    "        job_desc.append(job_desc_section.get_text())\n",
    "    else:\n",
    "        job_desc.append(\"Unknown\") \n",
    " \n",
    "    # getting the job description\n",
    "    job_req_section = job_soup.find('div', {'id':'skills-needed'})\n",
    "    if (job_req_section):\n",
    "        label_list = job_req_section.contents[1].contents\n",
    "        label_str = str()\n",
    "        j=0\n",
    "        for j in range(len(label_list)):\n",
    "            if (j>0):\n",
    "                label_str=label_str + \"|\" + label_list[j].get_text() \n",
    "        \n",
    "    if (label_str):\n",
    "        job_req.append(label_str)\n",
    "    else:\n",
    "        job_req.append(\"Unknown\") \n",
    "        \n",
    "    driver_job.close\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6883cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i=0  \n",
    "# initialise dataframe to hold the scrapping results\n",
    "data_df = pd.DataFrame()\n",
    "\n",
    "for url in job_url:\n",
    "#    if (i>10):\n",
    "#        break\n",
    "    #print(\"Company: \" + job.get_text())\n",
    "    #print(\"Job Title: \" + job_title[i])\n",
    "    #print(\"Job URL: \" + job_url[i])\n",
    "    #print(\"Job Location: \" + job_location[i])\n",
    "    #print(\"Job Type: \" + job_type[i])\n",
    "    #print(\"Job seniority: \" + job_level[i])\n",
    "    #print(\"Job Category: \" + job_cat[i])\n",
    "    #print(\"Salary: \" + salary_min[i] +\" to \" + salary_max[i])\n",
    "    #print(\"Salary type: \" + salary_type[i])\n",
    "    #print (\"Desc: \" + job_desc[i])\n",
    "    #print (\"Req: \" + job_req[i] +\"\\n\")\n",
    " \n",
    "    # Process job description\n",
    "\n",
    "\n",
    "    job_posting = [{\n",
    "        'company' : company_name[i],\n",
    "        'job_title' : job_title[i],\n",
    "        'location' : job_location[i],\n",
    "        'min salary' : salary_min[i],\n",
    "        'max salary' : salary_max[i],\n",
    "        'salary_type' : salary_type[i],\n",
    "        'job_description' : job_desc[i],\n",
    "        'job_requirement' : job_req[i],\n",
    "        'url' : url\n",
    "    }]\n",
    "    i+=1\n",
    "    \n",
    "    # append the job posting to dataframe\n",
    "    temp_df = pd.DataFrame(job_posting)\n",
    "    data_df = pd.concat([data_df, temp_df], axis=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7682a473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17780, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "393bce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('data_wh_ft.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dabe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cb5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
